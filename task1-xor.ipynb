{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1: XOR** (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print('Using GPU(0):', physical_devices[0])\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1: Backpropagation through time (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider some input data $x_1, x_2$ and groud truth $p_1, p_2$ (the indices denote different time steps), a simple RNN network is shown in the following figure. \n",
    "\n",
    "![bptt](./img/bptt2.jpg)\n",
    "\n",
    "Here, $w_x, w_h, b_1, w, b_2 \\in R$ are scalar parameters. Loss function is **mean squared error (MSE)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO:</strong></font>\n",
    "\n",
    "Assume the input $(x_1, x_2) = (1, -1)$, ground truth $(p_1, p_2) = (0, 1)$, $h_0 = 0$, and $(w_x, w_h, b_1, w, b_2) = (1, -2, 3, 2, 1)$, derive both the forward and backward pass (keep $4$ digits after the decimal). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>Hint:</strong></font> The sigmoid function and its derivative\n",
    "\n",
    "$$\n",
    "\\sigma (x) = \\frac{1}{1 + e^{-x}}, \\quad\n",
    "\\nabla_x \\sigma (x) = \\frac{e^{-x}}{1 + e^{-x}} = \\sigma (x) (1 - \\sigma (x))\n",
    "$$\n",
    "\n",
    "- Derive the equations and intermediate variables first before plugging in values, don't just fill in the answers\n",
    "- Use latex style equations\n",
    "- You derivations don't need to be wrapped with special color :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>Asnwer:</strong></font>\n",
    "\n",
    "$$\\text{Your Derivations Here.}$$\n",
    "$(x_1, x_2) = (1, -1)$\n",
    "$(p_1, p_2) = (0, 1)$, and $(w_x, w_h, b_1, w, b_2) = (1, -2, 3, 2, 1) \\\\ $\n",
    "$h_0 = 0 \\\\ $ \n",
    "$h_1 = \\sigma (w_x*x_1 + w_h*h_0 + b1) = \\sigma (4) = 0.982 \\\\ $\n",
    "$y_1 = \\sigma (w*h_1 + b2) = \\sigma (2.964) = 0.9509 \\\\ $\n",
    "$h_2 = \\sigma (w_x*x_2 + w_h*h_1 + b1) = \\sigma (0.036) = 0.509 \\\\ $\n",
    "$y_2 = \\sigma (w*h_2 + b2) = \\sigma (2.0179) = 0.8827 \\\\ $\n",
    "\n",
    "- Loss function for time t\n",
    "\n",
    "$ L_t = 1/2(y_t -p_t)^2 \\\\ $ \n",
    "$$ \\frac{\\partial L_1}{\\partial w_x} = \\frac{\\partial L_1}{\\partial y_1} \\frac{\\partial y_1}{\\partial h_1} \\frac{\\partial h_1}{\\partial w_x} \\\\ $$\n",
    "$$\\frac{\\partial L_1}{\\partial w_x} = (y_1 - p_1)*y_1(1- y_1)*w*h_1(1- h_1)*x_1$$\n",
    "$$\\frac{\\partial L_1}{\\partial w_x} = 0.0016$$\n",
    "$$ \\frac{\\partial L_2}{\\partial w_x} = \\frac{\\partial L_2}{\\partial y_2} \\frac{\\partial y_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial w_x} \\\\ $$\n",
    "$$\\frac{\\partial L_2}{\\partial w_x} = (y_2 - p_2)*y_2(1- y_2)*w*h_2(1- h_2)*x_2$$\n",
    "$$\\frac{\\partial L_2}{\\partial w_x} = 0.0063$$\n",
    "$$ \\nabla_{w_x} L = \\frac{\\partial L_1}{\\partial w_x}  + \\frac{\\partial L_2}{\\partial w_x} $$\n",
    "$$ \\nabla_{w_x} L = 0.0079$$\n",
    "\n",
    "$$ \\frac{\\partial L_1}{\\partial w_h} = \\frac{\\partial L_1}{\\partial y_1} \\frac{\\partial y_1}{\\partial h_1} \\frac{\\partial h_1}{\\partial w_h} \\\\ $$\n",
    "$$ \\frac{\\partial L_1}{\\partial w_h} = (y_1 - p_1)*y_1(1- y_1)*w*h_1(1- h_1)*h_0$$\n",
    "$$ \\frac{\\partial L_1}{\\partial w_h} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial L_2}{\\partial w_h} = \\frac{\\partial L_2}{\\partial y_2} \\frac{\\partial y_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial w_h} \\\\ $$\n",
    "$$ \\frac{\\partial L_2}{\\partial w_h} = (y_2 - p_2)*y_2(1- y_2)*w*h_2(1- h_2)*h_1$$\n",
    "$$ \\frac{\\partial L_2}{\\partial w_h} = -0.0060$$\n",
    "$$\\nabla_{w_h} L = \\frac{\\partial L_1}{\\partial w_h} + \\frac{\\partial L_2}{\\partial w_h} $$\n",
    "$$\\nabla_{w_h} L = -0.0060$$\n",
    "\n",
    "$$ \\frac{\\partial L_1}{\\partial b_1} = \\frac{\\partial L_1}{\\partial y_1} \\frac{\\partial y_1}{\\partial h_1} \\frac{\\partial h_1}{\\partial b_1} \\\\ $$\n",
    "$$ \\frac{\\partial L_1}{\\partial b_1} = (y_1 - p_1)*y_1(1- y_1)*w*h_1(1- h_1)$$\n",
    "$$ \\frac{\\partial L_1}{\\partial b_1} = 0.0016$$\n",
    "\n",
    "$$ \\frac{\\partial L_2}{\\partial b_1} = \\frac{\\partial L_2}{\\partial y_2} \\frac{\\partial y_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial b_1} $$\n",
    "$$ \\frac{\\partial L_2}{\\partial b_1} = (y_2 - p_2)*y_2(1- y_2)*w*h_2(1- h_2)$$\n",
    "$$ \\frac{\\partial L_2}{\\partial b_1} = -0.0060$$\n",
    "$$\\nabla_{b_1} L = \\frac{\\partial L_1}{\\partial b_1} + \\frac{\\partial L_2}{\\partial b_1} $$\n",
    "$$\\nabla_{b_1} L = -0.0044$$\n",
    "\n",
    "$$ \\frac{\\partial L_1}{\\partial w} = \\frac{\\partial L_1}{\\partial y_1} \\frac{\\partial y_1}{\\partial w} $$\n",
    "$$ \\frac{\\partial L_1}{\\partial b_1} = (y_1 - p_1)*y_1(1- y_1)*h_1$$\n",
    "$$ \\frac{\\partial L_1}{\\partial b_1} = 0.0436$$\n",
    " \n",
    "$$ \\frac{\\partial L_2}{\\partial w} = \\frac{\\partial L_2}{\\partial y_2} \\frac{\\partial y_2}{\\partial w} $$\n",
    "$$ \\frac{\\partial L_2}{\\partial w} = (y_2 - p_2)*y_2(1- y_2)*h_2$$\n",
    "$$ \\frac{\\partial L_2}{\\partial w} = 0.0062$$\n",
    "$$ \\nabla_{w} L = 0.0374$$\n",
    "\n",
    "$$ \\frac{\\partial L_1}{\\partial b_2} = \\frac{\\partial L_1}{\\partial y_1} \\frac{\\partial y_1}{\\partial b_2}  $$\n",
    "$$ \\frac{\\partial L_1}{\\partial b_2} = (y_1 - p_1)*y_1(1- y_1)$$\n",
    "$$ \\frac{\\partial L_1}{\\partial b_2} = 0.0016$$\n",
    "\n",
    "$$ \\frac{\\partial L_2}{\\partial b_2} = \\frac{\\partial L_2}{\\partial y_2} \\frac{\\partial y_2}{\\partial b_2} $$\n",
    "$$ \\frac{\\partial L_2}{\\partial b_2} = (y_2 - p_2)*y_2(1- y_2)$$\n",
    "$$ \\frac{\\partial L_2}{\\partial b_2} = 0.0443$$\n",
    "$$\\nabla_{b_2} L = \\frac{\\partial L_1}{\\partial b_2} + \\frac{\\partial L_2}{\\partial b_2} $$\n",
    "$$\\nabla_{b_2} L = 0.0322$$\n",
    "\n",
    "- The gradients\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\nabla_{w_x} L = \\color{cyan}{[0.0079]} \\\\\n",
    "\\nabla_{w_h} L = \\color{cyan}{[-0.0060]} \\\\\n",
    "\\nabla_{b_1} L = \\color{cyan}{[-0.0044]} \\\\\n",
    "\\nabla_{w} L = \\color{cyan}{[0.0374]} \\\\\n",
    "\\nabla_{b_2} L = \\color{cyan}{[0.0322]} \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified by tensorflow (TODO compare values with your hand-written calculations):\n",
      "dwx = 0.0079, dwh = -0.0060, db1 = -0.0043, dw = 0.0374, db2 = 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 09:04:19.911617: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# use tensorflow to verify the answer.\n",
    "with tf.GradientTape() as G:\n",
    "    wx = tf.Variable(1.0, dtype=tf.float64)\n",
    "    wh = tf.Variable(-2.0, dtype=tf.float64)\n",
    "    b1 = tf.Variable(3.0, dtype=tf.float64)\n",
    "    w = tf.Variable(2.0, dtype=tf.float64)\n",
    "    b2 = tf.Variable(1.0, dtype=tf.float64)\n",
    "\n",
    "    h0 = tf.Variable(0.0, dtype=tf.float64)\n",
    "    x = tf.Variable((1.0, -1.0), dtype=tf.float64)\n",
    "    p = tf.Variable((0.0, 1.0), dtype=tf.float64)\n",
    "\n",
    "    y = []\n",
    "    h1 = tf.sigmoid(wx*x[0] + wh*h0 + b1)\n",
    "    y.append(tf.sigmoid(w*h1 + b2))\n",
    "    h2 = tf.sigmoid(wx*x[1] + wh*h1 + b1)\n",
    "    y.append(tf.sigmoid(w*h2 + b2))\n",
    "\n",
    "    loss = 0.5*(tf.square(p[0]-y[0]) + tf.square(p[1]-y[1]))\n",
    "\n",
    "    dw_t, db2_t, dwx_t, dwh_t, db1_t = G.gradient(loss, [w, b2, wx, wh, b1])\n",
    "\n",
    "\n",
    "print(\"Verified by tensorflow (TODO compare values with your hand-written calculations):\")\n",
    "print(\"dwx = {:.4f}, dwh = {:.4f}, db1 = {:.4f}, dw = {:.4f}, db2 = {:.4f}\".format(dwx_t, dwh_t, db1_t, dw_t, db2_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use TensorFlow modules to create XOR network (15%)\n",
    "\n",
    "In this part, you need to build and train an XOR network that can learn the XOR function. It is a very simple implementation of RNN and will give you an idea how RNN is built and how to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR network\n",
    "\n",
    "![xnor_net](./img/xnor.png)\n",
    "\n",
    "XOR network can learn the XOR ($\\oplus$) function. If input \n",
    "\n",
    "$$\n",
    "(x_0, ..., x_7) = (0, 0, 1, 1, 1, 1, 1, 0)\n",
    "$$\n",
    "\n",
    "Then output should be\n",
    "\n",
    "$$\n",
    "(y_0, ..., y_7) = (0, 0, 1, 0, 1, 0, 1, 1)\n",
    "$$\n",
    "\n",
    "That is, \n",
    "\n",
    "$$\n",
    "y_n = x_0 \\oplus x_1 \\oplus ... \\oplus x_{n-1} \\oplus x_{n}\n",
    "$$\n",
    "\n",
    "It is also interesting to realize that the equation effectively equivalates to \n",
    "\n",
    "$$\n",
    "y_i = \\sum_{j = 0}^i x_j \\mod 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data set\n",
    "This function provides a way to generate the data which is needed for the training process. You should utilize it when building your training function for the GRU. Read the source code for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import create_xor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_xor_dataset(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network using a TensorFlow LSTMCell and GRUCell\n",
    "\n",
    "In this section, you are asked to build a XOR net using a TensorFlow LSTMCell and a GRUCell. In TensorFlow 2, these two cells are supported by Keras. Please check online documents below.\n",
    "\n",
    "Use TensorFlow to build and train your XOR net. The dataset is already provided. You will do the following:\n",
    "- Learn how to use `tf.keras.layers.LSTM` and `tf.keras.layers.GRU` in TensorFlow(Keras). \n",
    "- Choose appropriate parameters to build a model (Sequential Model in Keras is suggested). \n",
    "- Compile your model with appropriate loss function, optimizer, metrics, etc.\n",
    "- Train your model and see the loss history.\n",
    "\n",
    "Tips: \n",
    "1. Make sure that the shape of your data is corrrect after every step.\n",
    "2. Choose your loss function according to your network design.\n",
    "3. Choose 'accuracy' as your metrics when compiling your model.\n",
    "4. Make sure that names of history for the network with LSTMCell and GRUCell (which you used while training) are the same as the ones in the plotting functions.\n",
    "4. Feel free to ask TAs if you get stuck somewhere.\n",
    "\n",
    "Reference: \n",
    "1. [TensorFlow(Keras): Working with RNNs](https://keras.io/guides/working_with_rnns/)\n",
    "2. [TensorFlow: Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn)\n",
    "3. [TensorFlow LSTM Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "4. [TensorFlow GRU Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "5. [TensorFlow: Sequential Model](https://www.tensorflow.org/guide/keras/sequential_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "in_data, out_data = create_xor_dataset(1000) # create a dataset with a batch size of 1000\n",
    "print('Input data:', in_data.shape, in_data.dtype)\n",
    "print('Labels:', out_data.shape, out_data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting RNN Inputs\n",
    "\n",
    "Typically, the input data of an RNN has form of\n",
    "\n",
    "$$\n",
    "X \\in R^{T \\times N \\times D}\n",
    "$$\n",
    "\n",
    "where $T$ is ***the number of time steps***, $N$ is the batch size and $D$ is the dimension of each input. \n",
    "\n",
    "At each time step $t$, we feed \n",
    "\n",
    "$$\n",
    "X_t \\in R^{N \\times D}\n",
    "$$\n",
    "\n",
    "into the RNN to generate an output (i.e. the hidden state) $h_t \\in R^{N \\times K}$. \n",
    "\n",
    "Some tasks only take interest in the final output $h_T$, while others need the output from all of the time steps as \n",
    "\n",
    "$$\n",
    "H = [h_1, \\dots, h_T] \\in R^{T \\times N \\times K}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE:</strong></font>\n",
    "\n",
    "In real-life RNN models, the orders of $T$ and $N$ are actually exchangable by specifying certain model parameters, which is `time_major` in Tensorflow and `batch_first` in Pytorch. \n",
    "\n",
    "Use whatever you like in your implementations, but make sure to have a consistent input shape. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Answer the question: \n",
    "\n",
    "We talked about two usages of RNN model: \n",
    "\n",
    "- Return only $h_T$\n",
    "- Return all $H = [h_1, \\dots, h_T]$\n",
    "\n",
    "In the context of our XOR function, which one should we use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "In case of XOR function, we will use:\n",
    "  - Return all $H = [h_1, \\dots, h_T]$ where each $h_T$ for each input bit. In the LSTM Keras code, the parameter return_sequences=True enables this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>HINT:</strong></font>\n",
    "\n",
    "Upon answering the question above, think about how this is done with Tensorflow. Please reference [`LSTM` documents](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint for creating a model with LSTM\n",
    "\n",
    "```\n",
    "model = tf.keras.Sequential() \n",
    "model.add([LAYER1])\n",
    "model.add([LAYER2])\n",
    "model.add([...])\n",
    "model.add([OUTPUT_LAYER])\n",
    "\n",
    "model.summary() \n",
    "model.compile(loss=[LOSS_FUNCTION], optimizer=[OPTIMIZER], metrics=['accuracy'])\n",
    "\n",
    "history_LSTM = model.fit(in_data, out_data, batch_size=64, epochs=15) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Build a LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# TODO: build a network with LSTM and train it    #\n",
    "#                                                 #\n",
    "###################################################\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(128, input_shape=(8, 1), return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history_LSTM = model.fit(in_data, out_data, batch_size=64, epochs=15, validation_split=0.2)\n",
    "\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "in_data, out_data = create_xor_dataset(1000) # create a dataset with a batch size of 1000\n",
    "print('Input data:', in_data.shape, in_data.dtype)\n",
    "print('Labels:', out_data.shape, out_data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Build a GRU Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# TODO: build a model with GRU and train it.      #\n",
    "#                                                 #\n",
    "###################################################\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.GRU(128, input_shape=(8, 1), return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history_GRU = model.fit(in_data, out_data, batch_size=64, epochs=15, validation_split=0.2)\n",
    "\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_LSTM.history['loss'], label='LSTM')\n",
    "plt.plot(history_GRU.history['loss'], label='GRU')\n",
    "plt.title('LSTM/GRU loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_LSTM.history['accuracy'], label='LSTM')\n",
    "plt.plot(history_GRU.history['accuracy'], label='GRU')\n",
    "plt.title('LSTM/GRU accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Answer the question:\n",
    "\n",
    "Which part of this task have you been struggling with most of the  time? Describe how you resolved it.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "I had a hard time understanding LSTM architecture and its implementation. Thus, I read many articles on LSTM and Keras' implementation documents to understand better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Answer the question:\n",
    "\n",
    "Which loss function did you use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "I used the binary cross entropy. Each output can be either 0 or 1. Thus binary cross entropy is the correct loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 :  Build your own LSTMCell (25%)\n",
    "\n",
    "In this part, you need to build your own LSTM Cell to achieve the LSTM functionality (including different types of gates that constitute the cell).\n",
    "\n",
    "It is recommanded to see the course slides and [`LSTM` source code](https://github.com/keras-team/keras/blob/v2.10.0/keras/layers/rnn/lstm.py). [This link](https://colah.github.io/posts/2015-08-Understanding-LSTMs) also provides a good intuition. \n",
    "\n",
    "![](./img/lstm_cell.png)\n",
    "\n",
    "**Note that this is a simplified figure and not all operations are illustrated.** Please refer to the lectures and follow the equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume some **old** carry state $c_{t-1}$ and hidden state $h_{t-1}$, we try to compute the **new states** given some input $X_t$ using the ideas of LSTM. \n",
    "\n",
    "Remember that for an LSTM cell, we actually have (yellow boxes from left to right) a **forget gate**, an **input gate**, a **cell** and an **output gate**, whose output can be denoted by $f_{t-1}$, $i_{t-1}$, $\\tilde{c}_{t-1}$ and $o_{t-1}$. Each yellow box in the graph denotes a linear projection followed by a particular activation (`sigmoid` or `tanh`). \n",
    "\n",
    "Then for some input $X_t \\in R^{N \\times D}$ and a target output dimension $K$ (also called \"units\"), let's first combine the inputs with the old hidden state $h_{t-1} \\in R^{N \\times K}$ by projecting them into the same dimension \n",
    "\n",
    "$$\n",
    "Z_t = X_t + h_{t-1} W_h\n",
    "$$\n",
    "\n",
    "where $W_h \\in R^{K \\times D}$. Then all new states can be computed one by one as \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "f_t = \\sigma (Z_t W_f) \\\\\n",
    "i_t = \\sigma (Z_t W_i) \\\\\n",
    "\\tilde{c}_t = \\tanh (Z_t W_c) \\\\\n",
    "o_t = \\sigma (Z_t W_o)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where the weights (also called kernels)\n",
    "\n",
    "$$\n",
    "W_f, W_i, W_c, W_o \\in R^{D \\times K}\n",
    "$$\n",
    "\n",
    "Then the **new** carry state is given by the **old carry state** passing through the forget gate and combining with the input\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "And the hidden state is given by the **new carry state** passing through the output gate\n",
    "\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh (c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when computing each state, we are actually computing the four linear projections on $Z_t$, which is **four** projections on $X_t$ and **eight** (becuase of the $W_h$) projections on $h_{t-1}$. This doesn't look smart enough, so let's rewrite the equations. \n",
    "\n",
    "First define a kernel $W \\in R^{D \\times 4 K}$, which can be thought of as\n",
    "\n",
    "$$\n",
    "W \\gets [W_f, W_i, W_c, W_o]\n",
    "$$\n",
    "\n",
    "Similarly define a recurrent kernel $W_r \\in R^{K \\times 4 K}$, which is equivalent to \n",
    "\n",
    "$$\n",
    "W_r \\gets W_h W = [W_h W_f, W_h W_i, W_h W_c, W_h W_o]\n",
    "$$\n",
    "\n",
    "\n",
    "We can, of course, also add a bias $b \\in R^{4 K}$, and now all the projections simplifies to \n",
    "\n",
    "$$\n",
    "Z_t = X_t W + h_{t-1} W_r + \\mathbb{1} b^T \\in R^{N \\times 4 K}\n",
    "$$\n",
    "\n",
    "We do exactly **one** projection on $X_t$ and **one** on $h_{t-1}$. Then we partition $Z_t$ into \n",
    "\n",
    "$$\n",
    "[z_t^f, z_t^i, z_t^c, z_t^o] \\gets Z_t, \\quad\n",
    "z_t^* \\in R^{N \\times K}\n",
    "$$\n",
    "\n",
    "Easily, \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "f_t = \\sigma (z_t^f) \\\\\n",
    "i_t = \\sigma (z_t^i) \\\\\n",
    "\\tilde{c}_t = \\tanh (z_t^c) \\\\\n",
    "o_t = \\sigma (z_t^o)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "And the rest stays the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> \n",
    "\n",
    "1. Complete the model `LSTMCell` in **utils/LSTM.py**\n",
    "2. Verify the function with Tensorflow by running the provided code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__NOTE:__</span> \n",
    "\n",
    "You should use the \"simplified\" (second) set of equations as tensorflow does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple verification:\n",
      "Is h correct? True\n",
      "Is c correct? True\n"
     ]
    }
   ],
   "source": [
    "# Veirification Code\n",
    "# Please don't change anything\n",
    "\n",
    "from utils.LSTM import LSTMCell\n",
    "\n",
    "batch_size = 4\n",
    "input_dim = 8\n",
    "units = 64\n",
    "\n",
    "inputs = tf.random.normal((batch_size, input_dim))\n",
    "states = [tf.random.normal((batch_size, units)), tf.zeros((batch_size, units))]\n",
    "\n",
    "# By default, the weights will be initialized randomly (as is specified in the code). \n",
    "# Here we try to enforce deterministic (non-random) initializers \n",
    "# so that the two results can be correctly compared. \n",
    "# NOTE: You should NOT do this and leave them to default during actual training\n",
    "\n",
    "lstm_cell = LSTMCell(\n",
    "    units, \n",
    "    kernel_initializer=tf.keras.initializers.Ones, \n",
    "    recurrent_initializer=tf.keras.initializers.Ones, \n",
    "    bias_initializer=tf.keras.initializers.Zeros\n",
    ")\n",
    "h, (_, c) = lstm_cell(inputs, states)\n",
    "\n",
    "lstm_cell_tf = tf.keras.layers.LSTMCell(\n",
    "    units, \n",
    "    kernel_initializer='ones', \n",
    "    recurrent_initializer='ones', \n",
    "    bias_initializer='zeros'\n",
    ")\n",
    "h_tf, (_, c_tf) = lstm_cell_tf(inputs, states)\n",
    "\n",
    "print('Simple verification:')\n",
    "print('Is h correct?', np.allclose(h.numpy(), h_tf.numpy()))\n",
    "print('Is c correct?', np.allclose(c.numpy(), c_tf.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data, out_data = create_xor_dataset(1024)# create a dataset with a batch size of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 8, 1) (1024, 8)\n"
     ]
    }
   ],
   "source": [
    "print(in_data.shape, out_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint on building a model with your own LSTMCell\n",
    "\n",
    "When creating an RNN with your own custom cell, you can just plug in your `LSTMCell` as a building block into a basic `RNN` layer, specify other parameters (e.g. input_shape) as needed, and add it into your model as you did with other layers before. \n",
    "\n",
    "```\n",
    "self.rnn = tf.keras.layers.RNN([YOUR_CELL], **OTHER_ARGS)\n",
    "```\n",
    "\n",
    "For details, please refer to https://www.tensorflow.org/guide/keras/rnn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Complete the model `LSTMModel` in **utils/LSTM.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__NOTE:__</span> \n",
    "\n",
    "We are trying to use a custom layer (our `LSTMCell`) with multiple `call` arguments (\"inputs\" and \"states\"). The pre-building of these types of models is currently NOT allowed in tensorflow. \n",
    "\n",
    "So be sure NOT to call `model.summary()`. Compile and fit it directly and the building will be automatically done in runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Train your `LSTMModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 3s 43ms/step - loss: 0.6936 - accuracy: 0.1086 - val_loss: 0.6929 - val_accuracy: 0.2621\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.6930 - accuracy: 0.1759 - val_loss: 0.6928 - val_accuracy: 0.1942\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.6929 - accuracy: 0.2215 - val_loss: 0.6924 - val_accuracy: 0.2524\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.6928 - accuracy: 0.2649 - val_loss: 0.6926 - val_accuracy: 0.2621\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.6926 - accuracy: 0.2769 - val_loss: 0.6925 - val_accuracy: 0.2621\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.6909 - accuracy: 0.3116 - val_loss: 0.6869 - val_accuracy: 0.5437\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.6564 - accuracy: 0.5147 - val_loss: 0.6114 - val_accuracy: 0.5049\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5910 - accuracy: 0.6406 - val_loss: 0.5430 - val_accuracy: 0.7864\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5281 - accuracy: 0.6971 - val_loss: 0.5042 - val_accuracy: 0.7670\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4945 - accuracy: 0.7405 - val_loss: 0.4768 - val_accuracy: 0.8058\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4796 - accuracy: 0.7210 - val_loss: 0.4618 - val_accuracy: 0.8155\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4673 - accuracy: 0.7579 - val_loss: 0.4522 - val_accuracy: 0.7670\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4466 - accuracy: 0.7448 - val_loss: 0.4546 - val_accuracy: 0.9029\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4401 - accuracy: 0.7296 - val_loss: 0.4316 - val_accuracy: 0.8641\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4282 - accuracy: 0.7514 - val_loss: 0.4258 - val_accuracy: 0.7864\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4174 - accuracy: 0.7416 - val_loss: 0.4188 - val_accuracy: 0.8350\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4087 - accuracy: 0.7622 - val_loss: 0.4074 - val_accuracy: 0.8350\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4048 - accuracy: 0.7459 - val_loss: 0.4016 - val_accuracy: 0.8155\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3976 - accuracy: 0.7622 - val_loss: 0.4067 - val_accuracy: 0.8544\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.3924 - accuracy: 0.7557 - val_loss: 0.3890 - val_accuracy: 0.8155\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3844 - accuracy: 0.7503 - val_loss: 0.3853 - val_accuracy: 0.8544\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.3894 - accuracy: 0.7600 - val_loss: 0.4311 - val_accuracy: 0.8155\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.3807 - accuracy: 0.7557 - val_loss: 0.3759 - val_accuracy: 0.8544\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3687 - accuracy: 0.7622 - val_loss: 0.3794 - val_accuracy: 0.8544\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.3646 - accuracy: 0.7546 - val_loss: 0.3688 - val_accuracy: 0.8544\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3680 - accuracy: 0.7676 - val_loss: 0.3859 - val_accuracy: 0.8155\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.3670 - accuracy: 0.7666 - val_loss: 0.3554 - val_accuracy: 0.8252\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.3563 - accuracy: 0.7535 - val_loss: 0.3606 - val_accuracy: 0.8252\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.3627 - accuracy: 0.7514 - val_loss: 0.3567 - val_accuracy: 0.8835\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.3557 - accuracy: 0.7666 - val_loss: 0.3605 - val_accuracy: 0.8252\n"
     ]
    }
   ],
   "source": [
    "from utils.LSTM import LSTMModel\n",
    "\n",
    "###################################################\n",
    "# TODO: Instantiate your own model and train it.  #\n",
    "###################################################\n",
    "batch_size = 8\n",
    "units = 128\n",
    "output_dim = out_data.shape[1]\n",
    "input_shape = (in_data.shape[1], in_data.shape[2])\n",
    "activation = 'relu'\n",
    "model = LSTMModel(units, output_dim, activation, input_shape)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(in_data, out_data, batch_size=64, epochs=30, validation_split=0.1)\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='Val loss')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
